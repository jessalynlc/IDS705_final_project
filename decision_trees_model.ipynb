{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec5f3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a13c8566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        interval_start_local    date  hour  north_x_load  south_load  \\\n",
      "0  2018-01-01 00:00:00-06:00  1/1/18     0      21198.80    14046.47   \n",
      "1  2018-01-01 01:00:00-06:00  1/1/18     1      21176.60    14176.73   \n",
      "2  2018-01-01 02:00:00-06:00  1/1/18     2      21234.65    14265.94   \n",
      "3  2018-01-01 03:00:00-06:00  1/1/18     3      21365.95    14360.66   \n",
      "4  2018-01-01 04:00:00-06:00  1/1/18     4      21691.43    14516.28   \n",
      "\n",
      "   west_x_load  houston_load  total_load  lmp_HB_BUSAVG  coal_and_lignite  \\\n",
      "0      4759.33      10584.52    50589.13      27.285833       15806.62550   \n",
      "1      4722.19      10569.91    50645.44      27.427500       15903.98768   \n",
      "2      4675.70      10559.50    50735.79      27.243333       15913.71428   \n",
      "3      4673.38      10608.76    51008.76      27.770000       15932.35813   \n",
      "4      4698.25      10793.93    51699.89      28.697500       15904.12724   \n",
      "\n",
      "   ...  natural_gas  other_gen  coast_temp  east_temp  far_west_temp  \\\n",
      "0  ...  25122.23091   2.103656        39.4       28.5           20.0   \n",
      "1  ...  25351.36782   1.712454        38.3       27.5           19.5   \n",
      "2  ...  25177.21264   2.239718        37.3       25.5           19.5   \n",
      "3  ...  25678.39066   1.608887        36.8       25.0           19.0   \n",
      "4  ...  26451.29842   1.757122        35.0       24.0           18.5   \n",
      "\n",
      "   north_y_temp  north_central_temp  south_central_temp  southern_temp  \\\n",
      "0          17.5               22.75                27.5           42.2   \n",
      "1          16.5               21.75                27.0           40.6   \n",
      "2          16.0               21.25                27.0           39.2   \n",
      "3          15.5               20.25                26.5           38.2   \n",
      "4          14.5               19.00                26.5           36.8   \n",
      "\n",
      "   west_y_temp  \n",
      "0         18.2  \n",
      "1         17.6  \n",
      "2         17.6  \n",
      "3         17.0  \n",
      "4         16.6  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "all_hourly_data = pd.read_csv('all_hourly_data.csv')\n",
    "print(all_hourly_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544be65d",
   "metadata": {},
   "source": [
    "Trying out the following:\n",
    "- Binary regression decision trees\n",
    "- Bagged regression trees\n",
    "- Boosted regression trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "343feb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hourly_data['interval_start_local'] = pd.to_datetime(all_hourly_data['interval_start_local'], utc = True)\n",
    "\n",
    "# Extract time-based features\n",
    "all_hourly_data['hour'] = all_hourly_data['interval_start_local'].dt.hour\n",
    "all_hourly_data['dayofweek'] = all_hourly_data['interval_start_local'].dt.dayofweek\n",
    "all_hourly_data['month'] = all_hourly_data['interval_start_local'].dt.month\n",
    "all_hourly_data['year'] = all_hourly_data['interval_start_local'].dt.year\n",
    "all_hourly_data['is_weekend'] = all_hourly_data['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "#Train-Test Split (random 70% of the dataset as the training set, remaining as the test set)\n",
    "\n",
    "# Define X (features) and y (target - lmp_HB_BUSAVG)\n",
    "X = all_hourly_data.drop(columns=['lmp_HB_BUSAVG', 'interval_start_local', 'date'])\n",
    "y = all_hourly_data['lmp_HB_BUSAVG']                 \n",
    "\n",
    "# 70/30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91cdc7",
   "metadata": {},
   "source": [
    "Single Regression Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00a5be54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of splits: 40104.10\n",
      "Setting max_leaf_nodes = 20053\n",
      "\n",
      "Best min_samples_leaf: 15 with average RMSE: 136.2694\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "#Estimate average number of splits in unrestricted trees\n",
    "num_splits_list = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    tree = DecisionTreeRegressor(random_state=42)\n",
    "    tree.fit(X_tr, y_tr)\n",
    "\n",
    "    # Number of internal nodes = number of splits\n",
    "    tree_structure = tree.tree_\n",
    "    num_splits = tree_structure.node_count - tree_structure.n_leaves\n",
    "    num_splits_list.append(num_splits)\n",
    "\n",
    "avg_splits = np.mean(num_splits_list)\n",
    "print(f\"Average number of splits: {avg_splits:.2f}\")\n",
    "\n",
    "#Set limit on max number of splits as half the average\n",
    "max_leaf_nodes_limit = int((avg_splits / 2) + 1)  # +1 to convert to leaves\n",
    "print(f\"Setting max_leaf_nodes = {max_leaf_nodes_limit}\")\n",
    "\n",
    "#Test min_samples_leaf from 1 to 50\n",
    "rmse_per_leaf_size = []\n",
    "\n",
    "for min_leaf in range(1, 51):\n",
    "    fold_rmse = []\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        tree = DecisionTreeRegressor(\n",
    "            min_samples_leaf=min_leaf,\n",
    "            max_leaf_nodes=max_leaf_nodes_limit,\n",
    "            random_state=42\n",
    "        )\n",
    "        tree.fit(X_tr, y_tr)\n",
    "        preds = tree.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        fold_rmse.append(rmse)\n",
    "    \n",
    "    avg_rmse = np.mean(fold_rmse)\n",
    "    rmse_per_leaf_size.append((min_leaf, avg_rmse))\n",
    "\n",
    "#Find best parameter\n",
    "best_leaf, best_rmse = min(rmse_per_leaf_size, key=lambda x: x[1])\n",
    "print(f\"\\nBest min_samples_leaf: {best_leaf} with average RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ccf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagged regression trees\n",
    "\n",
    "oob_errors = []\n",
    "\n",
    "#Loop over different number of trees\n",
    "for n_trees in range(1, 101):\n",
    "    bagged_model = BaggingRegressor(\n",
    "        base_estimator=DecisionTreeRegressor(),  # unpruned trees\n",
    "        n_estimators=n_trees,\n",
    "        oob_score=True,\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all cores for faster training\n",
    "    )\n",
    "    \n",
    "    bagged_model.fit(X_train, y_train)\n",
    "\n",
    "    #OOB prediction is only available after fitting and when oob_score=True\n",
    "    if hasattr(bagged_model, 'oob_prediction_'):\n",
    "        oob_mae = mean_absolute_error(y_train, bagged_model.oob_prediction_)\n",
    "        oob_errors.append((n_trees, oob_mae))\n",
    "\n",
    "oob_df = pd.DataFrame(oob_errors, columns=[\"n_trees\", \"oob_mae\"])\n",
    "\n",
    "#Find the best number of trees\n",
    "best_n_trees = oob_df.loc[oob_df['oob_mae'].idxmin()]\n",
    "print(f\"\\nBest number of trees: {int(best_n_trees['n_trees'])} with OOB MAE: {best_n_trees['oob_mae']:.4f}\")\n",
    "\n",
    "#Plot the OOB error vs. number of trees\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(oob_df['n_trees'], oob_df['oob_mae'], marker='o')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('OOB MAE')\n",
    "plt.title('OOB MAE vs. Number of Bagged Trees')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76321308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosted regression trees\n",
    "# Define a range of learning rates to test\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "n_estimators = 200  # number of boosting stages (trees)\n",
    "max_splits = 10     # max number of splits per tree (so max_depth=log2(max_splits + 1))\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = GradientBoostingRegressor(\n",
    "        learning_rate=lr,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=int(np.log2(max_splits + 1)),  # Convert number of splits to depth\n",
    "        loss='squared_error',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    \n",
    "    results.append((lr, rmse))\n",
    "\n",
    "#Find the best learning rate\n",
    "best_lr, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(f\"\\nBest learning rate: {best_lr} with RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "#Plot\n",
    "lrs, rmses = zip(*results)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(lrs, rmses, marker='o')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.title('Boosted Regression Trees: Learning Rate vs RMSE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42eb36",
   "metadata": {},
   "source": [
    "Comparing the different trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451623b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Decision Tree model (best from CV)\n",
    "tree_model = DecisionTreeRegressor(\n",
    "    min_samples_leaf=best_leaf,\n",
    "    max_leaf_nodes=max_leaf_nodes_limit,\n",
    "    random_state=42\n",
    ")\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "#Final bagged model (best from OOB MAE tuning)\n",
    "bagged_model = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=best_n_trees,  # based on OOB MAE tuning\n",
    "    oob_score=True,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagged_model.fit(X_train, y_train)\n",
    "\n",
    "# Final Boosted Tree model (best from tuning)\n",
    "boosted_model = GradientBoostingRegressor(\n",
    "    learning_rate=best_lr,\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=int(np.log2(max_splits + 1)),\n",
    "    loss='squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "boosted_model.fit(X_train, y_train)\n",
    "\n",
    "#Predict on training data\n",
    "tree_preds_train = tree_model.predict(X_train)\n",
    "bagged_preds_train = bagged_model.predict(X_train)\n",
    "boosted_preds_train = boosted_model.predict(X_train)\n",
    "\n",
    "#Compare MAE\n",
    "tree_mae = np.sqrt(mean_absolute_error(y_train, tree_preds_train))\n",
    "bagged_mae = np.sqrt(mean_absolute_error(y_train, bagged_preds_train))\n",
    "boosted_mae = np.sqrt(mean_absolute_error(y_train, boosted_preds_train))\n",
    "\n",
    "#Compute RMSE\n",
    "tree_rmse = np.sqrt(mean_squared_error(y_train, tree_preds_train))\n",
    "bagged_rmse = np.sqrt(mean_squared_error(y_train, bagged_preds_train))\n",
    "boosted_rmse = np.sqrt(mean_squared_error(y_train, boosted_preds_train))\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"Decision Tree\", \"Bagged Trees\", \"Boosted Trees\"],\n",
    "    \"Train RMSE\": [tree_rmse, bagged_rmse, boosted_rmse]\n",
    "})\n",
    "\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
